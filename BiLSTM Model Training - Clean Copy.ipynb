{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fca9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports - some redundancies may be present\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tensorflow import py_function, double\n",
    "import requests\n",
    "import gc\n",
    "import json\n",
    "import tensorflow_addons as tfa\n",
    "import random\n",
    "\n",
    "# Set up number of features \n",
    "numFeatures = 100*7*4\n",
    "\n",
    "# Checkpoint\n",
    "print(\"Checkpoint: initialization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aec19c",
   "metadata": {},
   "source": [
    "### Custom preprocessing layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PreprocessingLayer, self).__init__(**kwargs) \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.temporalPenalty = self.add_weight(name='temporalPenalty',\n",
    "                                    initializer=tf.keras.initializers.RandomUniform(minval=50, maxval=100),\n",
    "                                    shape=(1,),\n",
    "                                    trainable=True)\n",
    "\n",
    "        super(PreprocessingLayer, self).build(input_shape)\n",
    "\n",
    "    #create the layer operation here\n",
    "    def call(self, x):\n",
    "        # Reshape the data\n",
    "        x = keras.layers.Reshape((700, 4))(x)\n",
    "        \n",
    "        # Caclulate the distances and grab the temperatures\n",
    "        distances = tf.math.sqrt(tf.math.add_n([tf.math.square(x[:,:,0]), tf.math.square(x[:,:,1]), tf.math.multiply(self.temporalPenalty, tf.math.square(x[:,:,2]))]))\n",
    "        temperatures = x[:, :, 3]\n",
    "    \n",
    "        # Sort the data by distances\n",
    "        sortOrder = tf.argsort(distances, direction='ASCENDING')\n",
    "        sortOrder = keras.layers.Reshape((700,))(sortOrder)\n",
    "        sortedDistances = tf.gather(distances, sortOrder, batch_dims=1, axis=-1)\n",
    "        sortedTemperatures = tf.gather(temperatures, sortOrder, batch_dims=1, axis=-1)\n",
    "\n",
    "        # Prepare the features\n",
    "        features = tf.reshape(tf.stack([sortedTemperatures, sortedDistances], axis=2),  (-1, 1400, 1))\n",
    "        features = features[:, 0:200, :]\n",
    "        features = tf.convert_to_tensor(features)\n",
    "        return features\n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return (None, 100, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0bd9a0",
   "metadata": {},
   "source": [
    "## Model set up ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ede9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Preprocessing step\n",
    "    x_input = tf.keras.layers.Input(shape=(numFeatures, )) # or numFeatures // 2 for only temperatures\n",
    "    features = keras.layers.Reshape((700, 4))(x_input)\n",
    "    features = PreprocessingLayer()(features)\n",
    "    \n",
    "    # LSTM Step\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(features)\n",
    "    x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = tf.keras.models.Model(inputs = x_input, outputs = x, name = \"LSTMLite\")\n",
    "\n",
    "    # Model parameters\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='mae', optimizer=opt, metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e6688",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "modelCheckpoint = keras.callbacks.ModelCheckpoint(\"Models/LSTMLite.h5\", \n",
    "                                                  save_best_only=True, \n",
    "                                                  save_weights_only=True,\n",
    "                                                  monitor='val_loss', \n",
    "                                                  mode='min')\n",
    "\n",
    "# Set up parameters for storing history\n",
    "h = tf.keras.callbacks.History()\n",
    "loss = np.empty(0);\n",
    "val_loss = np.empty(0);\n",
    "trainable_weight = np.empty(0);\n",
    "\n",
    "# Load weights here if this is a second learning run\n",
    "#model.load_weights(\"Models/LSTMLite.h5\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Training step\n",
    "# Model was trained using manual epochs; it ran one epoch for each file loaded, then closed again\n",
    "# Intention was to maximise shuffling and support generalization\n",
    "for j in range(0, 100):\n",
    "    indexArray = list(range(0, 15))\n",
    "    # Shuffle open order for better generalization\n",
    "    random.shuffle(indexArray)\n",
    "    for i in indexArray:\n",
    "        \n",
    "        # If this isn't the first run, load the previous weights\n",
    "        if counter > 0:\n",
    "            model.load_weights(\"Models/LSTMLite2.h5\")\n",
    "\n",
    "        # Load the training set\n",
    "        X_train = np.load(\"X_train_AllN_test\" + str(i) + \".npy\", mmap_mode='r')\n",
    "        y_train = np.load(\"Y_train_AllN_test\" + str(i) + \".npy\", mmap_mode='r')\n",
    "\n",
    "        # Train\n",
    "        h = model.fit(X_train, y_train, batch_size=1024, validation_split=0.5, epochs=1, callbacks=[modelCheckpoint], shuffle=True)\n",
    "        print(model.trainable_variables[0])\n",
    "        \n",
    "        # Update tracking\n",
    "        loss = np.append(loss, h.history['loss'])\n",
    "        val_loss = np.append(val_loss, h.history['val_loss'])\n",
    "        trainable_weight = np.append(trainable_weight, model.trainable_variables[0])\n",
    "        \n",
    "        #Save the model\n",
    "        model.save_weights(\"Models/LSTMLite2.h5\")\n",
    "\n",
    "        counter = counter + 1;\n",
    "\n",
    "        del X_train, y_train\n",
    "        gc.collect()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(0)\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bce47",
   "metadata": {},
   "source": [
    "### Test the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d6f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "\n",
    "# Run the python garbage collector, which will delete old objects on the GPU.\n",
    "# This is useful after modifying the model and restarting training.\n",
    "gc.collect()\n",
    "\n",
    "# Set up arrays and load weights\n",
    "y_true = np.empty(0)\n",
    "y_pred = np.empty(0)\n",
    "model.load_weights(\"Models/LSTMLite2.h5\")\n",
    "\n",
    "# Loop through all testing files\n",
    "for i in range(0, 9):\n",
    "    print(\"Up to file \" + str(i))\n",
    "    \n",
    "    # Load the first testing tiles\n",
    "    X_test = np.load(\"X_test_AllN_\" + str(i) + \".npy\", mmap_mode='r')\n",
    "    y_test = np.load(\"Y_test_AllN_\" + str(i) + \".npy\", mmap_mode='r')\n",
    "\n",
    "    # Performance metrics\n",
    "    performance_metrics = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('metrics' + str(performance_metrics))\n",
    "\n",
    "    # Append the y values to the growing arrays above\n",
    "    y_pred = np.append(y_pred, model.predict(X_test))\n",
    "    y_true = np.append(y_true, y_test)\n",
    "    \n",
    "    del X_test, y_test\n",
    "    gc.collect()\n",
    "\n",
    "# Get the actual MAE and RMSE for the whole set\n",
    "# MAE\n",
    "maeActual = np.mean(np.absolute(y_pred-y_true))\n",
    "print(\"MAE in degC = \" + str(maeActual))\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(np.mean(np.square(y_pred-y_true)))\n",
    "print(\"RMSE in degC = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412ae43",
   "metadata": {},
   "source": [
    "## Plot the history ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fit\n",
    "plt.plot(h.history['loss'])\n",
    "plt.plot(h.history['val_loss'])\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
