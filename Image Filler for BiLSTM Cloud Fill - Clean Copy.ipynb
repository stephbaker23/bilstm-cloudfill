{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d18d5cf",
   "metadata": {},
   "source": [
    "## Initial set-up ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f29557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports - some redundancies may remain\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math\n",
    "from tensorflow import py_function, double\n",
    "import requests\n",
    "import gc\n",
    "import json\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tensorflow.keras.callbacks import History\n",
    "import netCDF4 as nc\n",
    "\n",
    "\n",
    "# Set up some important numbers\n",
    "lastLoss = 100000\n",
    "numFeatures = 100*7*4\n",
    "days = 7\n",
    "batchSize = 512\n",
    "numEpochs = 30\n",
    "numTestingFiles = 50\n",
    "# maxY = 40\n",
    "# minY = 0\n",
    "\n",
    "# Pseudorandom seeding for consistency\n",
    "seed = 6\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set up some important numbers\n",
    "numTrainingFiles = 137\n",
    "numRecordsPerFile = 100000\n",
    "numTestingFiles = 50\n",
    "latDim = 101\n",
    "longDim = 101\n",
    "imageSize = latDim*longDim\n",
    "minLat = -12\n",
    "maxLat = -10\n",
    "minLon = 145\n",
    "maxLon = 147\n",
    "\n",
    "# Save file prefix\n",
    "savePrefix = 'LSTMLite'\n",
    "\n",
    "# Checkpoint\n",
    "print(\"Checkpoint: initialization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39adc3",
   "metadata": {},
   "source": [
    "### Preprocessing and custom parameter set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cca7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PreprocessingLayer, self).__init__(**kwargs) \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.temporalPenalty = self.add_weight(name='temporalPenalty',\n",
    "                                    initializer=tf.keras.initializers.RandomUniform(minval=50, maxval=100),\n",
    "                                    shape=(1,),\n",
    "                                    trainable=True)\n",
    "\n",
    "        super(PreprocessingLayer, self).build(input_shape)\n",
    "\n",
    "    #create the layer operation here\n",
    "    def call(self, x):\n",
    "        # Reshape and normalise the data\n",
    "        x = keras.layers.Reshape((700, 4))(x)\n",
    "        x = x / [100, 100, 3, 40]\n",
    "        \n",
    "        # Calculate the distances using the custom sorting parameter\n",
    "        distances = tf.math.sqrt(tf.math.add_n([tf.math.square(x[:,:,0]), tf.math.square(x[:,:,1]), tf.math.multiply(self.temporalPenalty, tf.math.square(x[:,:,2]))]))\n",
    "       \n",
    "        # Collect the temperatures\n",
    "        temperatures = x[:, :, 3]\n",
    "                    \n",
    "        # Sort the data by distance\n",
    "        sortOrder = tf.argsort(distances, direction='ASCENDING')\n",
    "        sortOrder = keras.layers.Reshape((700,))(sortOrder)\n",
    "        sortedDistances = tf.gather(distances, sortOrder, batch_dims=1, axis=-1)\n",
    "        sortedTemperatures = tf.gather(temperatures, sortOrder, batch_dims=1, axis=-1)\n",
    "\n",
    "        # Prepare the features\n",
    "        features = tf.reshape(tf.stack([sortedTemperatures, sortedDistances], axis=2),  (-1, 1400, 1))\n",
    "        features = features[:, 0:200, :]\n",
    "        features = tf.convert_to_tensor(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d25c02",
   "metadata": {},
   "source": [
    "### Build the model and load in weights ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b452a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Preprocessing step\n",
    "    x_input = tf.keras.layers.Input(shape=(numFeatures, )) # or numFeatures // 2 for only temperatures\n",
    "    features = keras.layers.Reshape((700, 4))(x_input)\n",
    "    features = PreprocessingLayer()(features)\n",
    "    \n",
    "    # LSTM step\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(features)\n",
    "    x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    model = tf.keras.models.Model(inputs = x_input, outputs = x, name = \"LSTMLite64\")\n",
    "\n",
    "    # Set up model parameters\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='mae', optimizer=opt, metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "# Create the model - can load in weights if this is a transfer learning run\n",
    "model = create_model()\n",
    "model.summary()\n",
    "model_name = \"LSTMLite64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check temporal weight\n",
    "model.load_weights(\"Models/LSTMLite64v2.h5\")\n",
    "print(model.trainable_weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09963321",
   "metadata": {},
   "source": [
    "### Load in files ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80bfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up\n",
    "gc.collect()\n",
    "\n",
    "# Load in the model weights\n",
    "model.load_weights #(Path to model goes here)\n",
    "\n",
    "# Create arrays for populating\n",
    "y_true_no_nan = np.empty(0)\n",
    "y_true = np.empty(0)\n",
    "pixel_locations = np.empty(0)\n",
    "y_pred_no_nan = np.empty(0)\n",
    "y_pred = np.empty(0)\n",
    "\n",
    "# Loop through all .npy files\n",
    "for i in range(0, 9):\n",
    "    print(\"Up to file \" + str(i))\n",
    "    \n",
    "    # Load the first testing tiles - note that paths may need to be changed!\n",
    "    X_test_no_nan = np.load(\"Data/X_test_AllN_\" + str(i) + \".npy\", mmap_mode='r')\n",
    "    y_test_no_nan = np.load(\"Data/Y_test_AllN_\" + str(i) + \".npy\", mmap_mode='r')\n",
    "    X_test = np.load(\"Data/X_test_with_nan_AllN_\" + str(i) + \".npy\", mmap_mode='r')\n",
    "    Y_test = np.load(\"Data/Y_test_with_nan_AllN_\" + str(i) + \".npy\", mmap_mode='r')\n",
    "    locations = np.load(\"Data/Position_test_AllN_\" + str(i) + \".npy\", mmap_mode='r')\n",
    "    print(\"I am loaded\") # Checkpoint\n",
    "    \n",
    "    # Append the y values to the growing arrays above\n",
    "    y_pred = np.append(y_pred, model.predict(X_test)*40)\n",
    "    print(\"I have predicted\") # Checkpoint\n",
    "    y_pred_no_nan = np.append(y_pred_no_nan, model.predict(X_test_no_nan)*40)\n",
    "    y_true = np.append(y_true, Y_test)\n",
    "    y_true_no_nan = np.append(y_true_no_nan, y_test_no_nan)\n",
    "    \n",
    "    # Check error so far\n",
    "    print(\"Error so far: \" + str(np.mean(np.absolute(y_pred_no_nan-y_true_no_nan))))  \n",
    "    \n",
    "    # Append the positional values\n",
    "    if i == 0:\n",
    "        pixel_locations = locations\n",
    "    else:\n",
    "        pixel_locations = np.append(pixel_locations, locations, axis=0)\n",
    "        print(locations)\n",
    "        print(pixel_locations)\n",
    "    \n",
    "    # Purge variables\n",
    "    del X_test, Y_test, X_test_no_nan, y_test_no_nan, locations\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Checkpoint: Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627eb7fc",
   "metadata": {},
   "source": [
    "### Fill the images ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testingInputNc = # Path to \"TestingDataFrom2020.nc\" goes here\n",
    "testingOutputNc = # Path to \"TestingTargetsFrom2020.nc\" goes here\n",
    "savePrefix = # Save path prefix goes here\n",
    "\n",
    "# Read in the testing images\n",
    "imageDataset = nc.Dataset(testingInputNc)\n",
    "targetDataset = nc.Dataset(testingOutputNc)\n",
    "sst = imageDataset['sst'][:]\n",
    "sstTarget = targetDataset['sst'][:]\n",
    "\n",
    "# Read in times\n",
    "timeFilename = # Path to \"TestingTimes2020.txt\" goes here\n",
    "timeFile = open(timeFilename)\n",
    "timeFileLines = timeFile.readlines()\n",
    "times = [int(s) for s in timeFileLines[0].split(',')]\n",
    "clouds = [int(s) for s in timeFileLines[1].split(',')]\n",
    "\n",
    "print(\"Checkpoint: about to make some images\")\n",
    "\n",
    "for imageIndex in range(0, len(times)):\n",
    "\n",
    "    # Find target index\n",
    "    targetLoc = np.nonzero(np.array(times)==times[imageIndex])\n",
    "    firstOccurance = int(targetLoc[0][0])\n",
    "    targetIndex = int(np.floor(firstOccurance/5))\n",
    "    print(targetIndex)\n",
    "\n",
    "    # Info about the image\n",
    "    testTime = times[imageIndex]\n",
    "    testCloud = clouds[imageIndex]\n",
    "\n",
    "    # Get up cloud image and save it in same format\n",
    "    testImage = sst[imageIndex, :, :]\n",
    "    np.savetxt(savePrefix + 'Day' + str(testTime) + 'Cloud' + str(testCloud) + '_Cloud.txt', testImage)\n",
    "\n",
    "    # Get the target image\n",
    "    targetImage = sstTarget[targetIndex, :, :]\n",
    "\n",
    "    # Image to be filled\n",
    "    testFilledImage = sst[imageIndex, :, :]\n",
    "\n",
    "    # Find image start\n",
    "    startIndex = 0\n",
    "    stopIndex = 0\n",
    "    for i in range(0, len(pixel_locations)):\n",
    "        if all(pixel_locations[i, 2:] == np.array([testTime, testCloud])):\n",
    "            startIndex = i\n",
    "            stopIndex = startIndex + imageSize\n",
    "            if stopIndex > len(y_pred):\n",
    "                stopIndex = len(y_pred) - 1\n",
    "                \n",
    "            break\n",
    "\n",
    "    # Loop through every pixel in each image\n",
    "    for x in range(0, longDim):\n",
    "        for y in range(0, latDim):\n",
    "            if math.isnan(testFilledImage[x, y]):\n",
    "                pixelLoc = np.array([x+1, y+1, testTime, testCloud]) # Adjustment in indices because of Matlab v Python\n",
    "                \n",
    "                for i in range(startIndex, stopIndex):\n",
    "                    if all(pixel_locations[i, :] == pixelLoc[:]):\n",
    "                        testFilledImage[x, y] = y_pred[i]\n",
    "\n",
    "    np.savetxt(savePrefix + 'Day' + str(testTime) + 'Cloud' + str(testCloud) + '_Target.txt', targetImage)\n",
    "    np.savetxt(savePrefix + 'Day' + str(testTime) + 'Cloud' + str(testCloud) + '_Filled.txt', testFilledImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770ec21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401147a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
